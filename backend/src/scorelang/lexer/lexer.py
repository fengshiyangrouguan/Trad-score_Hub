import re
import toml
from typing import List, Dict, Any


class Lexer:
    """
    ğŸ¼ Pipa è®°è°±è¯­è¨€è¯æ³•åˆ†æå™¨ï¼ˆæ”¯æŒçŠ¶æ€æœº + æ•´è¡Œæ‰«æï¼‰
    - æ”¯æŒ { ... } scoreunit å—
    - æ”¯æŒ next_state / pop_state
    - æ¯è¡Œå¯å¤š token
    """

    def __init__(self, config_path: str):
        self.config_path = config_path
        self.token_rules = self._load_rules()
        self.state_stack = ["normal"]

    # ---------------------------------------------
    # è½½å…¥ TOML é…ç½®
    # ---------------------------------------------
    def _load_rules(self) -> List[Dict[str, Any]]:
        data = toml.load(self.config_path)
        rules = []
        for token in data.get("TOKENS", []):
            pattern = token["pattern"]
            flags = 0
            if "IGNORECASE" in token.get("flags", "").upper():
                flags |= re.IGNORECASE
            compiled = re.compile(pattern, flags)
            rules.append({
                "name": token["name"],
                "compiled": compiled,
                "semantic": token.get("semantic"),
                "group1": token.get("group1", False),
                "group2": token.get("group2", False),
                "state": token.get("state", "any"),
                "next_state": token.get("next_state"),
                "pop_state": token.get("pop_state", False),
            })
        return rules

    # ---------------------------------------------
    # ä¸»ä½“è¯æ³•æ‰«æé€»è¾‘
    # ---------------------------------------------
    def tokenize(self, text: str) -> List[Dict[str, Any]]:
        tokens = []
        self.state_stack = ["normal"]
        lines = text.splitlines()

        for line_no, line in enumerate(lines, 1):
            line = line.strip()
            if not line:
                continue

            pos = 0
            while pos < len(line):
                remaining = line[pos:]
                current_state = self.state_stack[-1]
                matched = False

                # éå†å…¨éƒ¨è§„åˆ™
                for rule in self.token_rules:
                    if rule["state"] not in ("any", current_state):
                        continue

                    m = rule["compiled"].match(remaining)
                    if not m:
                        continue

                    # æ„é€  token
                    token_data = {
                        "type": rule["name"],
                        "semantic": rule.get("semantic"),
                        "value": m.group(1) if rule.get("group1") else m.group(0),
                        "lineno": line_no,
                    }
                    if rule.get("group2") and m.lastindex and m.lastindex >= 2:
                        token_data["extra"] = m.group(2)

                    tokens.append(token_data)

                    # --- çŠ¶æ€æ§åˆ¶ ---
                    if rule.get("next_state"):
                        self.state_stack.append(rule["next_state"])
                    elif rule.get("pop_state"):
                        if len(self.state_stack) > 1:
                            self.state_stack.pop()

                    # âœ… æ ¸å¿ƒå˜åŒ–ï¼š
                    # åœ¨ scoreunit çŠ¶æ€ä¸‹ï¼Œç»§ç»­æ‰«æè¿™ä¸€è¡Œï¼ˆä¸ breakï¼‰
                    pos += m.end()
                    matched = True

                    if current_state != "scoreunit":
                        # æ™®é€šçŠ¶æ€åŒ¹é…ä¸€ä¸ª token å°±è·³å‡ºè§„åˆ™å¾ªç¯
                        break
                    # å¦åˆ™ç»§ç»­æ‰«æåŒä¸€è¡Œä¸‹ä¸€ä¸ª tokenï¼ˆä¸ breakï¼‰

                if not matched:
                    # æœªåŒ¹é…ä»»ä½•è§„åˆ™
                    pos += 1  # è·³è¿‡ä¸€ä¸ªå­—ç¬¦ï¼Œé˜²æ­¢æ­»å¾ªç¯

        return tokens
